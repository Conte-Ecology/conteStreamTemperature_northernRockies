

Note: run 2-calcSpringFallBP before running this script

```{r load libs}
rm(list=ls())
gc()

library(ggplot2)
library(ggmcmc) # load before dplyr so later plyr doesn't override dplyr
library(dplyr)
library(DataCombine) # for the slide function
#library(nlme)
library(devtools)
install_github("Conte-Ecology/conteStreamTemperature")
library(conteStreamTemperature)


baseDir <- getwd()

dataInDir <- paste0(baseDir, '/dataIn/')
dataOutDir <- paste0(baseDir, '/dataOut/')
dataLocalDir <- paste0(baseDir, '/localData/')
graphsDir <- paste0(baseDir, '/graphs/')

load(paste0(dataOutDir, 'springFallBreakpoints.RData'))

# r Choose data source

#Northeast
CTDEP  <- F
MAFW   <- T
MAUSGS <- T
MADEP  <- T 
NHFG   <- F
NHDES  <- F
USFS   <- F
VTFWS  <- F
MEDMR  <- F

#Montana
MTUSGSYellowstone <- F
MTUSGSGlacier <- F

sourceChoice <- list( CTDEP,   MAFW,   MAUSGS, MADEP,   NHFG,   NHDES,   MEDMR,   USFS,   VTFWS,    MTUSGSYellowstone,   MTUSGSGlacier)
sourceNames  <- c   ('CTDEP', 'MAFW', 'MAUSGS', 'MADEP', 'NHFG', 'NHDES', 'MEDMR', 'USFS', 'VTFWS',  'MTUSGSYellowstone', 'MTUSGSGlacier')

dataSource <- sourceNames[sourceChoice == T]

fields <- c("agency", "date", "AgencyID", "year", "site", "date", "dOY", "temp", "airTemp", "prcp", "srad", "dayl", "swe")


var.names <- c("Latitude", "Longitude", "airTemp", "airTempLagged1", "airTempLagged2", "prcp", "prcpLagged1", "prcpLagged2", "prcpLagged3", "dOY", "Forest", "Herbacious", "Agriculture", "Developed", "TotDASqKM", "ReachElevationM", "ImpoundmentsAllSqKM", "HydrologicGroupAB", "SurficialCoarseC", "CONUSWetland", "ReachSlopePCNT", "srad", "dayl", "swe")

prepDataWrapper(var.names = var.names, dataInDir = dataInDir, dataOutDir = dataOutDir, file = paste0(dataOutDir, "tempDataSync-MA.Rdata"))
```





```{r}
tempData <- readStreamTempData(timeSeries=TRUE, covariates=TRUE, dataSourceList=dataSource, fieldListTS=fields, fieldListCD='ALL', directory=dataInDir)
springFallBPs$site <- as.character(springFallBPs$site)

# Join with break points
tempDataBP <- left_join(tempData, springFallBPs, by=c('site', 'year'))
rm(tempData) # save some memory

# Clip to syncronized season
tempDataSync <- filter(tempDataBP, dOY >= finalSpringBP & dOY <= finalFallBP)

# Creat site and year factor variables
tempDataSync$fyear <- as.factor(tempDataSync$year)
tempDataSync$fsite <- as.factor(tempDataSync$site)
```

```{r lag airTemp & prcp}

# Order by group and date
tempDataSync <- tempDataSync[order(tempDataSync$site,tempDataSync$year,tempDataSync$dOY),]

# For checking the order of tempDataSync
tempDataSync$count <- 1:length(tempDataSync$year)

tempDataSync <- tempDataSync[order(tempDataSync$count),] # just to make sure tempDataSync is ordered for the slide function

# airTemp
tempDataSync <- slide(tempDataSync, Var = "airTemp", GroupVar = "site", slideBy = -1, NewVar='airTempLagged1')
tempDataSync <- slide(tempDataSync, Var = "airTemp", GroupVar = "site", slideBy = -2, NewVar='airTempLagged2')

# prcp
tempDataSync <- slide(tempDataSync, Var = "prcp", GroupVar = "site", slideBy = -1, NewVar='prcpLagged1')
tempDataSync <- slide(tempDataSync, Var = "prcp", GroupVar = "site", slideBy = -2, NewVar='prcpLagged2')
tempDataSync <- slide(tempDataSync, Var = "prcp", GroupVar = "site", slideBy = -3, NewVar='prcpLagged3')

```

```{r}
# Make dataframe with just variables for modeling and order before standardizing
tempDataSync <- tempDataSync[ , c("agency", "date", "AgencyID", "year", "fyear", "site", "fsite", "date", "finalSpringBP", "finalFallBP", "FEATUREID", "HUC4", "HUC8", "HUC12", "temp", "Latitude", "Longitude", "airTemp", "airTempLagged1", "airTempLagged2", "prcp", "prcpLagged1", "prcpLagged2", "prcpLagged3", "dOY", "Forest", "Herbacious", "Agriculture", "Developed", "TotDASqKM", "ReachElevationM", "ImpoundmentsAllSqKM", "HydrologicGroupAB", "SurficialCoarseC", "CONUSWetland", "ReachSlopePCNT", "srad", "dayl", "swe")] #  

summary(tempDataSync)
dim(tempDataSync)
tempDataSync <- na.omit(tempDataSync) ####### Change this so don't take out NA in stream temperature?
dim(tempDataSync)
```

### Check variables for correlation
```{r correlation scatterplot matrix}
# check correlation among potential independent variables
# Cannot plot all points because will overload the plot and lock the system up - therefore thin first
pairs.full <- data.frame(lat = tempDataSync$Latitude,
                    lon = tempDataSync$Longitude,
                    airTemp = tempDataSync$airTemp, 
                    precip = tempDataSync$prcp,
                    drainage = tempDataSync$TotDASqKM,
                    forest = tempDataSync$Forest,
                    elevation = tempDataSync$ReachElevationM,
                    coarseness = tempDataSync$SurficialCoarseC,
                    wetland = tempDataSync$CONUSWetland,
                    impoundments = tempDataSync$ImpoundmentsAllSqKM,
                    swe = tempDataSync$swe,
                    dOY = tempDataSync$dOY, 
                    dOY2 = tempDataSync$dOY^2)

pairs.thin <- sample_n(pairs.full, 3000, replace = F)

# Move these into the package as helper functions--------
## put histograms on the diagonal
panel.hist <- function(x, ...)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y, col="gray", ...)
}

panel.cor <- function(x, y, digits=2, prefix="", cex.cor)
{
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r = (cor(x, y))
  txt <- format(c(r, 0.123456789), digits=digits)[1]
  txt <- paste(prefix, txt, sep="")
  if(missing(cex.cor)) cex <- 0.9/strwidth(txt)
  text(0.5, 0.5, txt, cex = cex) # * abs(r) # change color to red if >0.7
}
#--------

pairs(pairs.thin, upper.panel=panel.smooth, lower.panel=panel.cor, diag.panel=panel.hist)

# impoundments and drainage have some outliers. Look in more detail and reduce dataset to reflect this so inference is not based on a few sites in large drainages with dozens of impoundments. Could also check to make sure these are properly classified and not small tribuaries that get place in the main river (CT) catchment.
hist(tempDataSync$TotDASqKM)
dim(tempDataSync)
length(unique(tempDataSync$site))

tempDataSync <- filter(tempDataSync, filter = TotDASqKM <= 200)
hist(tempDataSync$TotDASqKM)
dim(tempDataSync)
length(unique(tempDataSync$site))

```
**Inference only on catchments with total drainage area <= 200 km^2

No problems of correlation among these potential independent covariates


### Separate data for fitting (training) and validation

```{r separate validation data}
#Use validation?
validate = T
  
#If validating:
  # Choose fraction of total # of sites:
validateFrac <- 0.2

if(validate) {
  n.fit <- floor(length(unique(tempDataSync$site)) * (1 - validateFrac))

  set.seed(2346)
  site.fit <- sample(unique(tempDataSync$site), n.fit, replace = FALSE) # select sites to hold back for testing 
  tempDataSyncValid <- subset(tempDataSync, !site %in% site.fit) # data for validation
  tempDataSync <- subset(tempDataSync, site %in% site.fit)    # data for model fitting (calibration)
  } else {
    tempDataSyncValid <- NULL
  }

```

```{r Standardize}
# Standardize for Analysis

tempDataSyncS <- cbind(tempDataSync[ ,c(1:15)],
             apply(X = tempDataSync[ ,16:dim(tempDataSync)[2]], MARGIN=2,
                   FUN = function(x){(x-mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE)}))



tempDataSyncValidS <- cbind(tempDataSyncValid[ ,c(1:15)],
             apply(X = tempDataSyncValid[ ,16:dim(tempDataSyncValid)[2]], MARGIN=2,
                   FUN = function(x){(x-mean(x, na.rm=TRUE)) / sd(x, na.rm=TRUE)}))

tempDataSyncValidS$fyear <- as.factor(tempDataSyncValidS$year)
tempDataSyncValidS$fsite <- as.factor(tempDataSyncValidS$site)

```

### Add unique deployment column and create vector to loop through for each unique site-deployment

```{r}
# Data for fitting
    tempDataSyncS <- indexDeployments(tempDataSyncS, regional = TRUE)
    firstObsRows <- createFirstRows(tempDataSyncS)
    evalRows <- createEvalRows(tempDataSyncS)

# Data for validation
    tempDataSyncValidS <- indexDeployments(tempDataSyncValidS, regional = TRUE)
    firstObsRowsValid <- createFirstRows(tempDataSyncValidS)
    evalRowsValid <- createEvalRows(tempDataSyncValidS)

```

```{r save tempDataSync for use in analysis}

save(tempDataSync, tempDataSyncS, tempDataSyncValid, tempDataSyncValidS, firstObsRows, evalRows, firstObsRowsValid, evalRowsValid, file=paste0(dataOutDir, 'tempDataSync.RData'))

```

Left out to save time:
# problem is what to do with the first 5 days of each deployment so we don't lose that data. Could do this for all the weather data then merge into the analysis data

----------------------------------------------------------------------------------------------------------------------------------------------------------------
# 5-day mean of prcp 
siteYearCombos <- unique(tempDataSync[,c('site','year')])

tempDataSync$prcp5Day <- NA

window <- 5
for (i in 1:nrow(siteYearCombos)){

  print(c(i,as.character(siteYearCombos$site[i]),siteYearCombos$year[i],i/nrow(siteYearCombos)))
  
  currSite <- which(tempDataSync$site == as.character(siteYearCombos$site[i]) & tempDataSync$year == siteYearCombos$year[i] )

  #Need this so sites with very short records don't crash the loop.
  if(length(currSite) >= window){currMean <-  zoo::rollapply(tempDataSync$prcp[currSite], width=window, fill=NA, mean, align = 'left')} else(currMean <- NA)
  
  tempDataSync$prcp5Day[currSite] <- currMean
}
----------------------------------------------------------------------------------------------------------------------------------------------------------------

